---
title: "Analysis of NPS/AD"
subtitle: 'Merge scRNA-seq freeze with meta-data freeze'
author: "Developed by [Gabriel Hoffman](http://gabrielhoffman.github.io/)"
date: "Run on `r Sys.time()`"
output: 
  html_document:
    toc: true
    smart: true
---


<!--- 

cd /sc/arion/projects/CommonMind/hoffman/NPS-AD/work/nps_ad/create_freezes/
ml python git pandoc gcc/11.2.0
git pull origin master
R --vanilla

system("git pull origin master"); rmarkdown::render("merge_with_metadata.Rmd");


# https://hoffmg01.hpc.mssm.edu/nps_ad/create_freezes/merge_with_metadata.html

# start high-mem interactive job to run merging
bsub -Is -q premium -R span[hosts=1] -R rusage[mem=60790] -W 12:00 -P acc_CommonMind -n 24 bash

# regular job to run this script
bsub -Is -q premium -R span[hosts=1] -R rusage[mem=10000] -W 12:00 -P acc_CommonMind -n 12 bash



--->

# Load packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning=FALSE,
  message=FALSE,
  error = FALSE,
  tidy = FALSE,
  dev = c("png", "pdf"),
  package.startup.message = FALSE,
  cache = FALSE,
  cache.lazy = FALSE)
```

```{r load.packages, cache=FALSE}
library(SingleCellExperiment)
library(zellkonverter)
library(DelayedArray)
library(HDF5Array)
library(dreamlet)
library(scater)
library(tidyverse)
library(kableExtra)
library(org.Hs.eg.db)

# update block size for reading h5ad file from disk
setAutoBlockSize(1e9)
```


# Merge H5AD with metadata
```{r merge_with_metadata}
merge_with_metadata = function(h5ad_file){
  # read raw/* from h5ad file
  sce_in = readH5AD(h5ad_file, use_hdf5=TRUE, raw=TRUE, verbose=FALSE, uns=FALSE)
  
  # use `raw` as counts
  sceCombine = swapAltExp(sce_in, "raw")
  rowData(sceCombine) = rowData(sce_in)
  rownames(sceCombine) = rownames(sce_in)
  reducedDim(sceCombine, "X_umap") = reducedDim(sce_in, "X_umap")
  reducedDim(sceCombine, "X_pca") = reducedDim(sce_in, "X_pca")
  reducedDim(sceCombine, "X_pca_regressed_harmony") = reducedDim(sce_in, "X_pca_regressed_harmony")
  counts(sceCombine) = assay(sceCombine, 'X')   # set counts assay to data in X
  assay(sceCombine, 'X') = NULL          # free X  

  # merge with full metadata
  #------------------------

  # Load metadata
  # file = "/sc/arion/projects/psychAD/NPS-AD/freeze2_rc/metadata/syn26527784_latest.csv"
  file = "/sc/arion/projects/psychAD/NPS-AD/freeze2_rc/metadata/clinical_metadata_full.csv"
  df_meta = read.csv(file)
  # rownames(df_meta) = df_meta$SubID

  # get order of matching
  i = match(sceCombine$SubID, df_meta$SubID)
  exclude = colnames(df_meta) %in% colnames(colData(sceCombine))

  # Assign new metadata
  colData(sceCombine) = cbind(colData(sceCombine), df_meta[i,!exclude,drop=FALSE])

  # new Bioassay info
  file = "/sc/arion/projects/psychAD/NPS-AD/freeze2_rc/metadata/Bioassay_Table_Tissue_Prep.tsv"
  df = read.table(file, header=TRUE, sep="\t")

  idx = match(sceCombine$SubID, df$true_SubID)

  sceCombine$BatchID = paste0("Set_", df$Set_number[idx])

  tab = table(sceCombine$BatchID, sceCombine$poolID)

  # Load data for batching and sequencing
  # df_batching = read_csv("/sc/arion/projects/psychAD/NPS-AD/freeze2_rc/metadata/NYGC_projID_date_poolID_flowcell_lane.csv", show_col_types=FALSE)
  # df_batching$date = as.character(df_batching$date)

  # # rename some batches
  # sceCombine$poolID = recode(sceCombine$poolID, 
  #     'NPSAD-145-S1' = "NPSAD-145-A1", 
  #     'NPSAD-145-S2' = "NPSAD-145-A2",
  #     'NPSAD-146-S1' = 'NPSAD-146-A1', 
  #     'NPSAD-146-S2' = 'NPSAD-146-A2')

  # # get order of matching
  # i = match(sceCombine$poolID, df_batching$poolID)
  # if( any(is.na(i)) ){
  #   msg = paste0(unique(sceCombine$poolID[which(is.na(i))]), collapse=" ")
  #   message(msg)
  #   stop("poolID missing")
  # }

  # Assign new metadata
  # cols = c('projID', 'date', 'pool', 'flowcell', 'lane')
  # colData(sceCombine) = cbind(colData(sceCombine), df_batching[i,cols])

  # only save genes with unique names
  tab = table(rownames(sceCombine))
  keep = rownames(sceCombine) %in% names(tab[tab==1])
  sceCombine = sceCombine[keep,]
  colData(sceCombine) = droplevels(colData(sceCombine))

  sceCombine
}

write_in_chunks = function(sce, outprefix){

  sceCopy = SingleCellExperiment( list(counts=counts(sce)),
          rowData = rowData(sce),
          colData = colData(sce)[,seq(ncol(colData(sce)))],
          reducedDims = reducedDims(sce))

  vec = seq(ncol(sceCopy))
  chunk_length <- 100000
  chunks = split(vec, ceiling(seq_along(vec) / chunk_length))

  res = lapply(names(chunks), function(id){
    message(id, '/', names(chunks)[length(chunks)])
    outfile = paste0(outprefix, '_chunk', sprintf("%02d", as.numeric(id)), ".h5ad")
    sceSub = sceCopy[,chunks[[id]]]
    writeH5AD(sceSub, outfile, compression="none", verbose=FALSE)
    })
}

sex_check = function(sce){

  sce$static = "all" 
  pb <- aggregateToPseudoBulk(sce,
        cluster_id = "static",
        sample_id  = "SubID",
        BPPARAM=SnowParam(1))

  # Process assays to compute log2 CPM
  res.proc = processAssays( pb, ~ 1, 
                    min.cells = 0,
                    min.count = 0,
                    min.samples = 0, 
                    min.prop = 0)
    
  # Extract merged expression and meta-data
  df = extractData(res.proc, "all")

  # Create a data.frame of UTY and XIST
  geneID = c("Row.names", "Sex", "XIST", "UTY")
  dfSub = df[,geneID] 
  dfSub$Sex = factor(dfSub$Sex, c("Male", "Female"))

  # predict sex based on gene expression
  fit = glm(Sex ~ XIST + UTY, dfSub, family="binomial")
  sex.prob = predict(fit, type="response")
   
  # score sex mislabeling
  dfSub$score = as.integer(dfSub$Sex) -1 - sex.prob

  dfSub
}


# zellkonverter::writeH5AD() fails with 1.5M cells
# So write in batches.
# Later, concatenate the chunks in python
# Do on high memory node
# Need to use AnnData > 0.8.0
write_merge_chunks_job = function(proj, outpref, timeTxt, jobfile){

  # write text file of chunks files
  infiles = dir(dirname(outpref), pattern = paste0(basename(outpref), "_chunk.*h5ad"), full.names=TRUE)
  h5ad_list = paste0(outprefix, "list_", basename(outpref), ".txt")
  write(infiles, file=h5ad_list)

  # Final H5AD
  outfile = paste0("/sc/arion/projects/psychAD/NPS-AD/freeze2_rc/h5ad_final/", proj, "_", timeTxt, ".h5ad")

  # write LSF job
  cmd = paste0("#!/bin/bash
#BSUB -J ", proj, "_", timeTxt, "
#BSUB -P acc_CommonMind
#BSUB -q premium
#BSUB -R rusage[mem=60790]
#BSUB -R span[hosts=1]
#BSUB -W 12:00
#BSUB -n 24
#BSUB -o %J.stdout         
#BSUB -eo %J.stderr   
#BSUB -L /bin/bash   

conda activate /hpc/users/hoffmg01/.cache/R/basilisk/1.8.0/0

alias python=/hpc/users/hoffmg01/.cache/R/basilisk/1.8.0/0/bin/python

SRC=/sc/arion/projects/CommonMind/hoffman/NPS-AD/work/nps_ad/concat_h5ad.py

python $SRC -i ", h5ad_list, " -o ", outfile)

  write(cmd, jobfile)
}

# write_merge_chunks_job( proj, outpref, timeTxt, jobfile )

# system("rm -f stdcombind; bsub < /sc/arion/scratch/hoffmg01/tmp/job_RUSH_2023-02-07_11_20.lsf")

# Frozen H5ADs
# https://docs.google.com/spreadsheets/d/1rE_I3Ez-NW8KNzDJQbiCjgRRc34TGzfPlLun-dzpxaE/edit#gid=1749951851
h5ad_files = c(
RUSH = "240124_PsychAD_freeze3_RUSH_clean.h5ad", 
HBCC = "240124_PsychAD_freeze3_HBCC_clean.h5ad",
MSSM = "240124_PsychAD_freeze3_MSSM_clean.h5ad",
FULL = "240124_PsychAD_freeze3_FULL_clean.h5ad",
AGING = "240124_PsychAD_freeze3_AGING_clean.h5ad")

path = "/sc/arion/projects/psychAD/NPS-AD/freeze2_proc/"
h5ad_files = sapply(h5ad_files, function(x) paste0(path, x))

outprefix = "/sc/arion/scratch/hoffmg01/tmp/"

figList = list()

for( proj in names(h5ad_files) ){

  message(proj)

  # read H5AD and merge with meta-data
  #-----------------------------------
  message('  reading h5ad...')
  sce = merge_with_metadata( h5ad_files[proj] )

  # QC
  #---  
  # message('  sex check...')
  # df = sex_check( sce )

  # figList[[proj]] = ggplot(df, aes(XIST, UTY, color=Sex)) +
  #     geom_point() +
  #     theme_classic() +
  #     theme(aspect.ratio=1, plot.title = element_text(hjust = 0.5)) +
  #     scale_color_manual(values=c("blue", "red")) +
  #       ggtitle(proj)

  # write sce in chunks
  #--------------------

  message('  writing chunks...')

  timeTxt = format(Sys.time(), "%Y-%m-%d_%H_%M")

  outpref = paste0(outprefix, proj, "_tmp_", timeTxt)

  write_in_chunks( sce, outpref )

  # write merge chunks code
  #------------------------

  jobfile = paste0("/sc/arion/scratch/hoffmg01/tmp/job_", proj, "_", timeTxt, ".lsf")
  write_merge_chunks_job( proj, outpref, timeTxt, jobfile )
}
```

```{r plots, eval=FALSE}
library(cowplot)

fig = plot_grid(plotlist = figList)
ggsave(file = "~/www/test.pdf", fig)
```



```{r combine, eval=FALSE}

# get interactive job
# -sla roussos_sla
bsub -Is -q premium -R span[hosts=1] -R rusage[mem=110000] -W 12:00 -P acc_CommonMind -n 12 bash

bsub -Is -q premium -R span[hosts=1] -R rusage[mem=60790] -W 12:00 -P acc_CommonMind -n 24 bash

#BSUB 

# convert basilisk version
sed -i 's/1.8.0/1.12.0/g' job*lsf

# run each merging
cd /sc/arion/scratch/hoffmg01/tmp/

for JOB in $(ls job*lsf);
do
  source <(cat $JOB | sed 's/^python//g')
done



cd /sc/arion/projects/psychAD/NPS-AD/freeze2_rc/h5ad_final/

mv /sc/arion/projects/psychAD/NPS-AD/freeze2_rc/h5ad_final/FULL_2023-09-12_18_32.h5ad /sc/arion/projects/psychAD/NPS-AD/freeze2_rc/h5ad_final/FULL_2023-09-12_18_32_orig.h5ad

# # fails
# ls job_RUSH_2023-02-07_11_20.lsf | parallel -P1 source

# # works
# source job_RUSH_2023-02-07_11_20.lsf
# something about starting a new shell
# using interactive lsf job works, but not automated job

# If H5AD is written using -ondisk, it is not compressed
# Perform compress afterward
conda activate /hpc/users/hoffmg01/.cache/R/basilisk/1.12.0/0

alias python=/hpc/users/hoffmg01/.cache/R/basilisk/1.12.0/0/bin/python

import anndata as ad
import h5py

infile = "/sc/arion/projects/psychAD/NPS-AD/freeze2_rc/h5ad_final/FULL_2023-09-12_18_32_orig.h5ad"
outfile = "/sc/arion/projects/psychAD/NPS-AD/freeze2_rc/h5ad_final/FULL_2023-09-12_18_32.h5ad"

# file-backed H5AD
adata = ad.read(infile, backed='r')

# Write to disk with compression
adata.write( outfile, compression="lzf" )

```












